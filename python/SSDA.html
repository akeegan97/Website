<!DOCTYPE HTML>

<html>
	<head>
		<title>Super Store Project</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a class="title">Super Store</a>
				<nav>
					<ul>
						<li><a href="../index.html">Back to Main</a></li>
						<li><a href="../Python.html">Back to Projects</Main></a></li>
						
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<Main>
					<section id = "main" class = "wrapper">
						<div class="inner">
					<h1 class="major">Super Store Data Analysis</h1><section id="main" class="wrapper"></section>
					<div class="inner">
					<p> The "Super Store Data Analysis" project embarked on an analytical journey through the historical sales data of a 
						hypothetical super store, utilizing a dataset obtained from Kaggle.com that spans several years. 
						The project's analytical endeavors were twofold. Initially, it delved into the geographical dimensions of sales, 
						aiming to uncover insights into the most significant destinations for product shipments and the regions generating the 
						highest profits. Subsequently, the analysis took a deeper dive into forecasting sales across the store's three primary 
						product categories. By leveraging Python for data analysis, the project aimed to equip the business with predictive insights, 
						enabling strategic inventory optimization for the forthcoming year and beyond.
					</p>
					<div>
					</div>
					</section>
					<section id = "main" class = "wrapper">
						<div class="inner">
					<h1 class="major">Exploratory Data Analysis</h1><section id="main" class="wrapper">


								<h3><a href= "https://public.tableau.com/views/Super_Store_Tableau/Dashboard2?:language=en-US&:display_count=n&:origin=viz_share_link" target = '_blank' >Tableau Visualization Link</a></h3>
								<p>
									In this project, I harnessed the power of Tableau to develop an array of insightful dashboards and 
									worksheets, meticulously designed to elucidate the dataset. These visual tools were instrumental in revealing 
									key insights, such as the extensive geographical footprint of the store, identifying the most profitable customer 
									segments, and uncovering other valuable data points. The visualizations crafted in Tableau not only enhanced the analytical 
									process but also provided a clear and intuitive means to comprehend complex data relationships and trends within the super 
									store's operational scope.
								</p>
		
								<h3>Georaphical Analysis</h3>
								<span class="image fit"><img src = "../images/MAP.jpg" alt ="" /></span>
						</div>
					</section>
					<section id="main" class="wrapper">
						<div class = "inner">
							<p>
								The geographical analysis revealed a surprising lack of correlation between the quantity of units sold and profit across 
								states, challenging initial expectations. For instance, California exemplified the anticipated relationship, leading in 
								both unit sales and profitability. However, this pattern was not universally observed. Texas, despite its high volume of 
								orders, emerged as notably less profitable, even incurring losses over the observed period. This intriguing divergence was 
								highlighted in the Geographic Dashboard on Tableau, showcasing that such counterintuitive relationships were not isolated 
								incidents but prevalent among several states, prompting a reevaluation of distribution and marketing strategies.
							</p>
						</div>
					</section>
					<section id = "main" class="wrapper">
						<div class="inner">
							<h3>Segment, Category, and Consumer Group Data</h3>
							<span class = "image fit"><img src = "../images/Profit_Orders_Segment_Category.jpg" alt ="" /></span>
							<p>
								In another insightful dashboard, I dissected profits and unit sales across Customer Segments, Categories, and Subcategories. 
								The Consumer segment emerged as the most lucrative and the largest in terms of units ordered, underscoring its significance to the store's 
								success. The dashboard further revealed Technology as the top-earning category, generating $145,000 in profits, followed by Office Supplies at 
								$120,000, and Furniture trailing with $18,000. These findings suggest a strategic pivot might be beneficial, advocating for an increased focus on 
								Technology and Office Supplies while considering a reduction in Furniture offerings.

								Additionally, the dashboard provided a granular view of profitability by Subcategory, offering precise insights into the store's 
								profit drivers. This level of detail enables targeted adjustments to the product mix, ensuring the store capitalizes on the most profitable 
								aspects of its business.
								
							</p>
							<h3>Conclusion</h3>
							<p>
								Leveraging Tableau for exploratory data analysis unveiled intriguing patterns, 
								notably the lack of a consistent positive correlation between the number of orders 
								and profit across different states. A deeper dive into Customer Segments, Categories, 
								and Subcategories shed light on crucial business dynamics. It was discovered that Consumers 
								constitute the most profitable Customer Segment, Technology leads as the highest-earning Category, 
								and within Subcategories, Copiers stand out for their exceptional profitability. The dashboard further 
								enriches this analysis, presenting additional insights that underscore the strategic areas of focus for 
								enhancing business outcomes.
							</p>
						</div>
					</section>
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Forecasting</h1>
							
							<div class = "inner">
							<span class="image fit"><img src="../images\Units_Sold_Forecast.jpg" alt="" /></span>
							<p>
								The subsequent phase of the project involved forecasting sales for the three primary categories: Technology, Furniture, and Office Supplies. 
								Given the pronounced seasonal trends observed in the data, I opted for a SARIMA (Seasonal Autoregressive Integrated Moving Average) model over a 
								traditional ARIMA model to more accurately capture these variations. </p>
								<p>To facilitate efficient analysis and forecasting, I utilized Python for data manipulation, organizing the data into three separate Pandas 
									dataframes corresponding to each category. This process included filtering out irrelevant columns and addressing any missing values. I employed the 
									statsmodels.api library, which offers comprehensive functionalities for constructing SARIMA time series models. To verify the seasonal nature of our 
									datasets, I implemented a function specifically designed to evaluate seasonality in each time series. The results for the Furniture category, depicted 
									in figure 1, unequivocally confirm the seasonal pattern of the data, with a notable sales peak in the fourth quarter.</p>
							<h3>Figure 1. Seasonality Decomposition<span class = "image fit"><img src = "../images/Seasonal_decomp_furniture.jpg" alt="" /></span> </h3>
							<p>Having established the seasonal nature of the data, the ensuing step was to assess the stationarity of the three time series. To achieve this, 
								I devised a function leveraging the adfuller function from the statsmodels.tsa.stattools library. This function, known as the Augmented Dickey-Fuller (ADF) 
								test, is designed to detect unit roots, thereby testing for stationarity in time series data. The optimal number of lagged periods was determined based on the 
								lowest Akaike Information Criterion (AIC) value. Remarkably, the ADF test results for the Technology, Furniture, and Office Supplies categories all demonstrated 
								stationarity, satisfying the criteria at the 90%, 95%, and 99% confidence levels.</p>
								<h3>Augmented Dickey-Fuller Test Code </h3>
								<pre><code>
def ADF_test(timeseries, dataDesc):
    print(' > Is the {} stationary ?'.format(dataDesc))
    dftest = adfuller(timeseries.dropna(), autolag='AIC')
    print('Test statistic = {:.3f}'.format(dftest[0]))
    print('P-value = {:.3f}'.format(dftest[1]))
    print('Critical values :')
    for k, v in dftest[4].items():
    print('\t{}: {} - the data is {} stationary with {}% confidence'.
	format(k,v,'not' if v < dftest[0] else '', 100-int(k[:-1])))
								</code></pre>
								<h3>ADF test results for Furniture<span class = "image fit"><img src = "../images\output_ADF_furniture.jpg" alt=""/></span></h3>
								<p>To fine-tune the SARIMA model for peak performance, I embarked on optimizing its parameters through a comprehensive grid search method. 
									This approach aims to identify the parameter combination that yields the lowest Akaike Information Criterion (AIC), thereby ensuring the most 
									efficient model. The parameter sets are denoted as (p,d,q)x(P,D,Q,m), where (p,d,q) represent the model's non-seasonal components—trend autoregression order, 
									differencing order, and moving average order, respectively—and (P,D,Q,m) symbolize the seasonal aspects of the model. This optimization process was meticulously 
									carried out for each of the three main categories, ensuring that each model was precisely tailored to its corresponding dataset.</p>
								<h3>Grid Search Code </h3>
								<pre>
<code>
def sarima_grid_search(y, seasonal_period):
	p = d = q  =range(0,2)
	pdq = list(itertools.product(p,d,q))
	seasonal_pdq = [(x[0],x[1],x[2], 
	seasonal_period) for x in list(itertools.product(p,d,q))]
								
	mini = float('+inf')
								
	for param in pdq:
		for param_seasonal in seasonal_pdq:
			try: 
				mod = sm.tsa.statespace.SARIMAX(y,
				order=param,
				seasonal_order=param_seasonal,
				enforce_stationarity=False,
				enforce_invertibility=False)
				results = mod.fit()
				if results.aic < mini:
					mini = results.aic
					param_mini = param
					param_seasonal_mini = param_seasonal
			except:
				continue
	print('the set of params with min AIC is SARIMA{}x{} - 
	AIC:{}'.format(param_mini, param_seasonal_mini, mini))

								</code> </pre>
								<h3>Output of gride search for the furniture category <span class = "image fit"><img src = "../images\output_of gridsearch.jpg" alt=""/></span></h3>

								<p>With the optimal SARIMA parameters identified, the subsequent step involved their application within the `sarima_eva` 
									function. This function requires the dataset, optimized parameters, forecast period, the cutoff date for the training dataset, 
									and the dataset for comparison as inputs. For this analysis, I selected the data spanning from December 2016 to December 2017 as the 
									comparison dataset, allowing for a precise evaluation of the model's forecasting accuracy against known outcomes.</p>
								<h3>SARIMA EVA Function Code</h3>
								<pre><code>
def sarima_eva(y, order, seasonal_order, seasonal_period, pred_date, y_to_test):
    mod = sm.tsa.statespace.SARIMAX(
        y, 
    order=order,
    seasonal_order=seasonal_order,
    enforce_invertibility=False,
    )
    results = mod.fit()
    print(results.summary().tables[1])

    pred = results.get_prediction(start=pd.to_datetime(pred_date),dynamic = False)
    pred_ci = pred.conf_int()
    y_forecasted = pred.predicted_mean
    mse = ((y_forecasted - y_to_test)**2).mean()
    print('The Root Mean Squared Error of Sarima w/ seasonal_length={} 
	and dynamic = False {}'.format(seasonal_period,round(np.sqrt(mse),2)))


    pred_dynamic = results.get_prediction(start = pd.to_datetime(pred_date), dynamic = True,
	full_results = True)
    pred_dynamic_ci = pred_dynamic.conf_int()
    y_forecasted_dynamic = pred_dynamic.predicted_mean
    mse_dynamic = ((y_forecasted_dynamic - y_to_test)**2).mean()
    print('The Root Mean Square Error of Sarima w/season_length = {}
	and dynamic = TRUE {}'.format(seasonal_period,round(np.sqrt(mse_dynamic),2)))

    return(results)
								</code></pre>
								<h3>Output of SARIMA EVA Function</h3><span class = "image fit"><img src ="../images\output_SARIMA_EVA_to_console.jpg" alt=""/> </span></p>
								<p>The `sarima_eva` function processes the input variables to forecast values for the year 2017, subsequently comparing these predictions with the 
									actual historical data. The function's output, articulated through print statements, provides critical insights into the model's performance. 
									It particularly highlights the Root Mean Squared Error (RMSE), also known as Root Mean Square Deviation, between the forecasted outcomes and the 
									real values. This comparison offers a quantitative measure of the accuracy of the two model variants produced by the function, enabling an evaluation 
									of their predictive reliability.</p>

									<p>The latter segment of the `sarima_eva` function is dedicated to visualizing the model's forecasting accuracy through various plots, 
										juxtaposing predicted values against actual historical data. Figure 2 presents diagnostic plots, including a Q-Q plot, standardized residuals, 
										histogram, and correlogram. The Q-Q plot confirms that residuals are normally distributed, and the correlogram indicates that a lag of 1 is 
										appropriate for the model's moving average component. Figures 3 and 4 contrast different model approaches with the observed data, showcasing 
										their respective accuracies.</p>
									<p>Figure 5 introduces the dynamic model, which recursively uses its own forecasts for future predictions, enhancing the complexity of 
										forecasting as it progresses through the timeline. Conversely, Figure 6 illustrates the one-step ahead model, which consistently uses actual 
										historical data for each subsequent forecast, inherently limiting its predictions to one period ahead. The comparative analysis reveals the one-step 
										ahead model's superior performance, aligning with expectations. Nevertheless, the dynamic model's competent forecasting capabilities render it a valuable 
										tool for preliminary analysis and planning.</p>

								<h3>Figure 2. Plot Diagnostics <span class = "image fit"><img src ="../images\sarima_eva_output_group_plots.jpg" alt="" /></span></h3>
								<h3> Figure 3. Dynamic Forecast <span class = "image fit"><img src = "../images\sarima_eva_dynamic_forecast_plot.jpg" alt="" /></span></h3>
								<h3> Figure 4. One-Step Forecast <span class = "image fit"><img src = "../images\sarima_eva_one_step_forecast_plot.jpg" alt="" /></span></h3>
								<p>The methodology detailed above, demonstrated through the Furniture category, was uniformly applied to the Technology and Office Supplies categories, 
									yielding analogous insights. The consistency of the results across all categories underscores the robustness and applicability of the SARIMA model for 
									this type of data analysis.</p>
								<p>Upon validating the model's accuracy against observed data, the next phase involved generating forward-looking forecasts. This entailed fitting the 
									model to the entire historical dataset to project sales and profits one year into the future. The procedure and the corresponding Python code are 
									illustrated in Figure 5, which details the forecasting and plotting functions. Figures 6 and 7 display the actual versus forecasted profits and units sold, 
									offering a visual representation of the model's predictive capacity and the anticipated financial and operational performance of the store.</p>
								<h3>Forecast Function and Plotting Function</h3>
								<pre><code>
def forecast(model, predict_steps, y):
	pred_uc = model.get_forecast(steps = predict_steps)
	pred_ci = pred_uc.conf_int()
	pm = pred_uc.predicted_mean.reset_index()
	pm.columns = ['Date', 'Predicted_Mean']
	pci = pred_ci.reset_index()
	pci.columns = ['Date', 'Lower Bound', 'Upper Bound']
	final_table = pm.join(pci.set_index('Date'), on = 'Date')
	final_table = pd.DataFrame(final_table)
								
	return(final_table)
								
								
#Function that displays the forecasted data:
								
def forecasted_plot(model,predict_steps,y):
	pred_uc = model.get_forecast(steps = predict_steps)
	pred_ci = pred_uc.conf_int()
	ax = y.plot(label = 'Observed', figsize= (14,7))
	pred_uc.predicted_mean.plot(ax=ax, label = "Forecast")
	ax.fill_between(pred_ci.index,
	pred_ci.iloc[:,0],
	pred_ci.iloc[:,1], color = 'k', alpha = .25)
	ax.set_xlabel('Date')
	ax.set_ylabel(y.name)
	plt.legend()
	plt.show()
								
								</code></pre>
								<h3>Figure 5. Profit Plot<span class = "image fit"><img src = "../images\Profit_Forecast.jpg" alt = ""/></span></h3>
								<h3>Figure 6. Quantity Plot<span class = "image fit"><img src = "../images\Units_Sold_Forecast.jpg" alt = ""/></span></h3>
								<p>The forecasted profits depicted in Figure 6 were calculated by applying the average profit per unit sold for 
									each category to the predicted volume of units sold. This approach, while based on assumptions such as 
									consistent product mix and stable profit margins, aims to provide a generalized forecast of category-specific profitability for the upcoming year.</p>
								<h3>Conclusion</h3>
								<p>The forecasting exercise confirmed the seasonal nature of sales, offering valuable insights for inventory 
									optimization and strategic marketing investment. Notably, the forecast predicts increased sales across all 
									categories, signaling healthy business growth. Leveraging these predictions and the insights garnered from the 
									exploratory data analysis can further enhance the growth trajectory, positioning the business to capitalize on 
									emerging opportunities and address underperforming areas effectively.</p>
						</div>
					</section>
				

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
					<a href="https://github.com/akeegan97/Superstore_Python_Project"target = "_blank">Link to Github Repository,</a></li>
					<a href= "https://public.tableau.com/views/Super_Store_Tableau/Dashboard2?:language=en-US&:display_count=n&:origin=viz_share_link" target = '_blank' >Tableau Visualization Link</a>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.scrollex.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>